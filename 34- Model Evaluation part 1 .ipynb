{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e2537c",
   "metadata": {},
   "source": [
    "<!-- # Model evaluation\n",
    "\n",
    "Model evaluation is a crucial step in the machine learning process that helps assess the performance and effectiveness of a trained model. It involves various metrics and techniques to measure how well the model generalizes to new, unseen data. The main goal of model evaluation is to ensure that the model is accurate, reliable, and suitable for its intended purpose. Here's an overview of some key aspects of model evaluation:\n",
    "\n",
    "**1. Train-Test Split:**\n",
    "- The dataset is typically split into two subsets: the training set and the test set.\n",
    "- The training set is used to train the model, while the test set is used to evaluate its performance on unseen data.\n",
    "\n",
    "**2. Cross-Validation:**\n",
    "- Cross-validation is a technique used to assess model performance by dividing the data into multiple subsets (folds).\n",
    "- The model is trained on several combinations of training and validation sets, and the performance is averaged to provide a more robust evaluation.\n",
    "\n",
    "**3. Evaluation Metrics:**\n",
    "- Various evaluation metrics are used based on the type of machine learning problem (classification, regression, etc.).\n",
    "- For classification, common metrics include accuracy, precision, recall, F1-score, and area under the ROC curve (AUC-ROC).\n",
    "- For regression, common metrics include mean squared error (MSE), root mean squared error (RMSE), and mean absolute error (MAE).\n",
    "\n",
    "**4. Confusion Matrix:**\n",
    "- The confusion matrix is used to evaluate the performance of a classification model.\n",
    "- It provides a summary of the true positive, true negative, false positive, and false negative predictions.\n",
    "\n",
    "**5. Bias-Variance Tradeoff:**\n",
    "- Model evaluation also considers the tradeoff between bias and variance.\n",
    "- High bias can result in underfitting, while high variance can lead to overfitting.\n",
    "- The goal is to strike a balance to achieve a model that generalizes well to new data.\n",
    "\n",
    "**6. Receiver Operating Characteristic (ROC) Curve:**\n",
    "- ROC curve is a graphical representation of a classification model's performance across different thresholds.\n",
    "- It plots the true positive rate (sensitivity) against the false positive rate (1-specificity).\n",
    "- The area under the ROC curve (AUC-ROC) is a commonly used metric for evaluating classification models.\n",
    "\n",
    "**7. Precision-Recall Curve:**\n",
    "- The precision-recall curve is another evaluation tool for classification models, especially when dealing with imbalanced datasets.\n",
    "- It plots precision against recall, showing the tradeoff between these two metrics at different thresholds.\n",
    "\n",
    "**8. Overfitting and Underfitting:**\n",
    "- Model evaluation helps detect overfitting (when the model performs well on the training data but poorly on the test data) and underfitting (when the model fails to capture the underlying patterns).\n",
    "\n",
    "**9. Hyperparameter Tuning:**\n",
    "- Model evaluation is often used to optimize hyperparameters, such as learning rate, number of hidden layers, etc., to improve model performance.\n",
    "\n",
    "**10. Model Comparison:**\n",
    "- Model evaluation allows for comparison between different models to determine the best-performing one for a specific problem.\n",
    "\n",
    "**11. Validation Set and Test Set:**\n",
    "- In addition to the train-test split, a validation set is sometimes used to fine-tune the model's hyperparameters without touching the test set, which is only used for the final evaluation.\n",
    "\n",
    "Remember that model evaluation is an iterative process, and it requires a good understanding of the data, the problem at hand, and the appropriate metrics to ensure that the model meets the desired performance criteria. It's essential to choose the right evaluation strategy and metrics based on the specific machine learning task and the nature of the data. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1e283",
   "metadata": {},
   "source": [
    "## Model Evaluation: Model Evaluation and Refinement\n",
    "\n",
    "Model Evaluation and Refinement is an iterative process in machine learning to assess the performance of a model, identify areas of improvement, and refine the model to achieve better results. It involves evaluating the model using various metrics, diagnosing potential issues, and making necessary adjustments to improve its performance. Here's an overview of the steps involved in Model Evaluation and Refinement:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94af0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f995950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'feature1': [1, 2, 3, 4, 5],\n",
    "    'feature2': [10, 20, 30, 40, 50],\n",
    "    'target': [0, 1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to CSV file\n",
    "df.to_csv('data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84039b",
   "metadata": {},
   "source": [
    "#### All the points for Model Evaluation and Refinement with examples\n",
    "**1. Train-Test Split:**\n",
    "- The dataset is divided into training and test sets.\n",
    "- The training set is used to train the model, and the test set is used to evaluate its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478278d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Splitting the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializing and training a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a9af7",
   "metadata": {},
   "source": [
    "\n",
    "**2. Model Training and Evaluation:**\n",
    "- The model is trained on the training set using a specific algorithm or technique.\n",
    "- The model is evaluated on the test set using appropriate evaluation metrics based on the type of machine learning task (classification, regression, etc.).\n",
    "\n",
    "The example is the same as in the \"Train-Test Split\" section, where we train a logistic regression model and evaluate its accuracy on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d353623c",
   "metadata": {},
   "source": [
    "**3. Evaluation Metrics:**\n",
    "- Various evaluation metrics are used to assess the model's performance.\n",
    "- For classification tasks, common metrics include accuracy, precision, recall, F1-score, and area under the ROC curve (AUC-ROC).\n",
    "- For regression tasks, common metrics include mean squared error (MSE), root mean squared error (RMSE), and mean absolute error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684ded7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Initializing and training a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e67a4af",
   "metadata": {},
   "source": [
    "\n",
    "**4. Cross-Validation:**\n",
    "- Cross-validation is a technique to validate the model's performance on different subsets of the data.\n",
    "- It helps detect overfitting and provides a more robust evaluation by averaging the performance across multiple folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "418b1fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.5 0.5 0. ]\n",
      "Mean Cross-Validation Score: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Initializing a support vector machine (SVM) model\n",
    "model = SVC()\n",
    "\n",
    "# Performing cross-validation with 5 folds\n",
    "scores = cross_val_score(model, X, y, cv=3)\n",
    "\n",
    "print(\"Cross-Validation Scores:\", scores)\n",
    "print(\"Mean Cross-Validation Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34383175",
   "metadata": {},
   "source": [
    "**5. Hyperparameter Tuning:**\n",
    "- Model performance can be affected by hyperparameters, which are set before training the model.\n",
    "- Hyperparameter tuning involves selecting the best combination of hyperparameters to optimize model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a08952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Initializing an SVM model\n",
    "model = SVC()\n",
    "\n",
    "# Hyperparameters to tune\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': [0.1, 1, 10]}\n",
    "\n",
    "# Performing grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8634eb",
   "metadata": {},
   "source": [
    "\n",
    "**6. Model Refinement:**\n",
    "- After evaluating the initial model, potential issues like overfitting or underfitting can be identified.\n",
    "- The model can be refined by adjusting hyperparameters, selecting different features, or changing the algorithm.\n",
    "\n",
    "Model refinement involves analyzing the evaluation metrics, detecting overfitting or underfitting, and adjusting hyperparameters or model complexity accordingly. Here's an example using a Decision Tree Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42af202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Initializing a Decision Tree Classifier model\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "# Training the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dafd68b",
   "metadata": {},
   "source": [
    "**7. Feature Engineering:**\n",
    "- Feature engineering involves creating new features or transforming existing ones to improve model performance.\n",
    "- It helps the model better capture patterns and relationships in the data.\n",
    "\n",
    "Feature engineering involves creating or transforming features to improve model performance. Here's a simple example of adding a new feature 'age_group' based on age:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d04f879d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'feature1': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'feature2': [10, 20, 30, 40, 50, 60, 70, 80],\n",
    "    'target': [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Convert the data dictionary into a DataFrame\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# Adding a new feature 'age_group'\n",
    "data['age_group'] = pd.cut(data['feature2'], bins=[0, 30, 50, 100], labels=['Young', 'Adult', 'Senior'])\n",
    "\n",
    "# One-hot encoding for 'age_group'\n",
    "data = pd.get_dummies(data, columns=['age_group'])\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = data.drop(['target', 'feature2'], axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Initializing a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Training the model on the training set\n",
    "model.fit(X, y)\n",
    "\n",
    "# Making predictions on the training set\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Evaluating the model's accuracy\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1da1c0",
   "metadata": {},
   "source": [
    "**8. Ensemble Methods:**\n",
    "- Ensemble methods combine multiple models to make predictions, which often result in improved performance.\n",
    "- Common ensemble methods include bagging (e.g., Random Forest) and boosting (e.g., Gradient Boosting Machines).\n",
    "\n",
    "Using a Bagging Classifier with a Decision Tree as the base estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb8f8f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Initializing a Decision Tree Classifier as the base estimator\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "\n",
    "# Initializing a Bagging Classifier ensemble model\n",
    "model = BaggingClassifier(base_estimator=base_estimator, n_estimators=10)\n",
    "\n",
    "# Training the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3fa677",
   "metadata": {},
   "source": [
    "**9. Model Selection:**\n",
    "- If multiple models are considered, model evaluation helps select the best-performing model for the specific problem.\n",
    "\n",
    "Model selection involves comparing multiple models to choose the best-performing one. Here's an example comparing a Decision Tree and a Random Forest Classifier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5251e7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.0\n",
      "Random Forest Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Initializing a Decision Tree Classifier model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Initializing a Random Forest Classifier model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Training both models on the training set\n",
    "dt_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluating both models' accuracy\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", accuracy_dt)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b74a8",
   "metadata": {},
   "source": [
    "**10. Validation Set:**\n",
    "- A validation set can be used for intermediate testing during the model refinement process.\n",
    "- It helps fine-tune the model's hyperparameters without touching the test set until the final evaluation.\n",
    "\n",
    "\n",
    "A validation set can be created using a similar approach as the train-test split in the \"Train-Test Split\" section, where a portion of the data is held out for validation during hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c705f3e",
   "metadata": {},
   "source": [
    "\n",
    "**11. Iterative Process:**\n",
    "- Model evaluation and refinement is an iterative process.\n",
    "- Multiple iterations may be required to achieve the desired level of performance.\n",
    "\n",
    "The iterative process of Model Evaluation and Refinement involves repeating the steps of training, evaluation, and refinement until the desired performance is achieved. Here's an example using Gradient Boosting Classifier with hyperparameter tuning:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dbb5807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Initializing a Gradient Boosting Classifier model\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# Hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Performing grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Initializing the model with the best hyperparameters\n",
    "final_model = GradientBoostingClassifier(**best_params)\n",
    "\n",
    "# Training the final model on the entire dataset\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Evaluating the final model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Final Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ceb042",
   "metadata": {},
   "source": [
    "\n",
    "**12. Final Evaluation:**\n",
    "- Once the model has been refined, it is evaluated on the test set one final time to get a realistic estimate of its performance on new, unseen data.\n",
    "\n",
    "\n",
    "After refining the model, it is evaluated one final time on the test set to assess its performance on new, unseen data. Here's an example using the tuned Gradient Boosting Classifier from the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77a11541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Initializing a Gradient Boosting Classifier model with tuned hyperparameters\n",
    "final_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "\n",
    "# Training the final model on the entire dataset\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Evaluating the final model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Final Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aacb0d",
   "metadata": {},
   "source": [
    "In this example, the tuned Gradient Boosting Classifier is trained on the entire dataset and evaluated on the test set one final time to estimate its performance on new, unseen data.\n",
    "\n",
    "\n",
    "Model Evaluation and Refinement is an essential part of the machine learning workflow, as it ensures that the model is reliable, accurate, and performs well on real-world data. Through continuous evaluation and refinement, the model's performance can be improved, leading to better predictions and more effective decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981fe21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
